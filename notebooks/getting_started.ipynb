{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cthb8O3LCPM1"
   },
   "source": [
    "# SynthID Text: Watermarking for Generated Text\n",
    "\n",
    "This notebook demonstrates how to use the [SynthID Text library][synthid-code]\n",
    "to apply and detect watermarks on generated text. It is divided into three major\n",
    "sections and intended to be run end-to-end.\n",
    "\n",
    "1.  **_Setup_**: Importing the SynthID Text library, choosing your model (either\n",
    "    [Gemma][gemma] or [GPT-2][gpt2]) and device (either CPU or GPU, depending\n",
    "    on your runtime), defining the watermarking configuration, and initializing\n",
    "    some helper functions.\n",
    "1.  **_Applying a watermark_**: Loading your selected model using the\n",
    "    [Hugging Face Transformers][transformers] library, using that model to\n",
    "    generate some watermarked text, and comparing the perplexity of the\n",
    "    watermarked text to that of text generated by the base model.\n",
    "1.  **_Detecting a watermark_**: Training a detector to recognize text generated\n",
    "    with a specific watermarking configuration, and then using that detector to\n",
    "    predict whether a set of examples were generated with that configuration.\n",
    "\n",
    "[gemma]: https://ai.google.dev/gemma/docs/model_card\n",
    "[gpt2]: https://huggingface.co/openai-community/gpt2\n",
    "[synthid-code]: https://github.com/google-deepmind/synthid-text\n",
    "[synthid-paper]: https://www.nature.com/\n",
    "[transformers]: https://huggingface.co/docs/transformers/en/index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "be-I0MNRbyWT"
   },
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install synthid-text[notebook] datasets huggingface_hub tensorflow tqdm transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "aq7hChW8njFo"
   },
   "outputs": [],
   "source": [
    "from collections.abc import Sequence\n",
    "import enum\n",
    "import gc\n",
    "\n",
    "import datasets\n",
    "import huggingface_hub\n",
    "from synthid_text import detector_mean\n",
    "from synthid_text import logits_processing\n",
    "from synthid_text import synthid_mixin\n",
    "from synthid_text import detector_bayesian\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import tqdm\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "w9a5nANolFS_"
   },
   "outputs": [],
   "source": [
    "class ModelName(enum.Enum):\n",
    "  GPT2 = 'gpt2'\n",
    "  GEMMA_2B = 'google/gemma-2b-it'\n",
    "  GEMMA_7B = 'google/gemma-7b-it'\n",
    "\n",
    "\n",
    "model_name = 'google/gemma-2b-it' # @param ['gpt2', 'google/gemma-2b-it', 'google/gemma-7b-it']\n",
    "MODEL_NAME = ModelName(model_name)\n",
    "\n",
    "if MODEL_NAME is not ModelName.GPT2:\n",
    "  huggingface_hub.notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "B_pe-hG6SW6H"
   },
   "outputs": [],
   "source": [
    "DEVICE = (\n",
    "    torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    ")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "UOGvCjyVjjQ5"
   },
   "outputs": [],
   "source": [
    "CONFIG = synthid_mixin.DEFAULT_WATERMARKING_CONFIG\n",
    "CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "79mekKj5UUZR"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "NUM_BATCHES = 320\n",
    "OUTPUTS_LEN = 1024\n",
    "TEMPERATURE = 0.5\n",
    "TOP_K = 40\n",
    "TOP_P = 0.99\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME.value)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "logits_processor = logits_processing.SynthIDLogitsProcessor(\n",
    "    **CONFIG, top_k=TOP_K, temperature=TEMPERATURE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "hndT3YCQUt6D"
   },
   "outputs": [],
   "source": [
    "def load_model(\n",
    "    model_name: ModelName,\n",
    "    expected_device: torch.device,\n",
    "    enable_watermarking: bool = False,\n",
    ") -> transformers.PreTrainedModel:\n",
    "  if model_name == ModelName.GPT2:\n",
    "    model_cls = (\n",
    "        synthid_mixin.SynthIDGPT2LMHeadModel\n",
    "        if enable_watermarking\n",
    "        else transformers.GPT2LMHeadModel\n",
    "    )\n",
    "    model = model_cls.from_pretrained(model_name.value, device_map='auto')\n",
    "  else:\n",
    "    model_cls = (\n",
    "        synthid_mixin.SynthIDGemmaForCausalLM\n",
    "        if enable_watermarking\n",
    "        else transformers.GemmaForCausalLM\n",
    "    )\n",
    "    model = model_cls.from_pretrained(\n",
    "        model_name.value,\n",
    "        device_map='auto',\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "  if str(model.device) != str(expected_device):\n",
    "    raise ValueError('Model device not as expected.')\n",
    "\n",
    "  return model\n",
    "\n",
    "\n",
    "def _process_raw_prompt(prompt: Sequence[str]) -> str:\n",
    "  \"\"\"Add chat template to the raw prompt.\"\"\"\n",
    "  if MODEL_NAME == ModelName.GPT2:\n",
    "    return prompt.decode().strip('\"')\n",
    "  else:\n",
    "    return tokenizer.apply_chat_template(\n",
    "        [{'role': 'user', 'content': prompt.decode().strip('\"')}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qs9Ih8r4Dyu5"
   },
   "source": [
    "# 2. Applying a watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "JJ28Aajwu9uD"
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "batch_size = 1\n",
    "example_inputs = [\n",
    "    'I enjoy walking with my cute dog',\n",
    "    'I am from New York',\n",
    "    'The test was not so very hard after all',\n",
    "    \"I don't think they can score twice in so short a time\",\n",
    "]\n",
    "example_inputs = example_inputs * (int(batch_size / 4) + 1)\n",
    "example_inputs = example_inputs[:batch_size]\n",
    "\n",
    "inputs = tokenizer(\n",
    "    example_inputs,\n",
    "    return_tensors='pt',\n",
    "    padding=True,\n",
    ").to(DEVICE)\n",
    "print(\"Loading model\")\n",
    "model = load_model(MODEL_NAME, expected_device=DEVICE, enable_watermarking=True)\n",
    "torch.manual_seed(0)\n",
    "print(\"Model loaded\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    max_length=1024,\n",
    "    top_k=40,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "print(\"Generation finished\")\n",
    "print('Output:\\n' + 100 * '-')\n",
    "for i, output in enumerate(outputs):\n",
    "  print(tokenizer.decode(output, skip_special_tokens=True))\n",
    "  print(100 * '-')\n",
    "\n",
    "del inputs, outputs, model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCBDURjUc-6a"
   },
   "source": [
    "# 3. Detecting a watermark\n",
    "\n",
    "To detect the watermark, we use for the moment one option:\n",
    "We use the simple **Mean** scoring function. This can be done quickly and requires no training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ya4rVfgDlKGf"
   },
   "outputs": [],
   "source": [
    "NUM_NEGATIVES = 10000\n",
    "POS_BATCH_SIZE = 32\n",
    "NUM_POS_BATCHES = 313\n",
    "NEG_BATCH_SIZE = 32\n",
    "# Truncate outputs to this length for training.\n",
    "POS_TRUNCATION_LENGTH = 200\n",
    "NEG_TRUNCATION_LENGTH = 200\n",
    "# Pad trucated outputs to this length for equal shape across all batches.\n",
    "MAX_PADDED_LENGTH = 1000\n",
    "TEMPERATURE = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "oD1x8ClskqVw"
   },
   "outputs": [],
   "source": [
    "def generate_responses(example_inputs, enable_watermarking):\n",
    "  inputs = tokenizer(\n",
    "      example_inputs,\n",
    "      return_tensors='pt',\n",
    "      padding=True,\n",
    "  ).to(DEVICE)\n",
    "\n",
    "  # Watermarked output preparation for detector training\n",
    "  gc.collect()\n",
    "  torch.cuda.empty_cache()\n",
    "\n",
    "  model = load_model(\n",
    "      MODEL_NAME,\n",
    "      expected_device=DEVICE,\n",
    "      enable_watermarking=enable_watermarking,\n",
    "  )\n",
    "  torch.manual_seed(0)\n",
    "  _, inputs_len = inputs['input_ids'].shape\n",
    "\n",
    "  outputs = model.generate(\n",
    "      **inputs,\n",
    "      do_sample=True,\n",
    "      max_length=inputs_len + OUTPUTS_LEN,\n",
    "      temperature=TEMPERATURE,\n",
    "      top_k=TOP_K,\n",
    "      top_p=TOP_P,\n",
    "      pad_token_id=tokenizer.eos_token_id,\n",
    "  )\n",
    "\n",
    "  outputs = outputs[:, inputs_len:]\n",
    "\n",
    "  # eos mask is computed, skip first ngram_len - 1 tokens\n",
    "  # eos_mask will be of shape [batch_size, output_len]\n",
    "  eos_token_mask = logits_processor.compute_eos_token_mask(\n",
    "      input_ids=outputs,\n",
    "      eos_token_id=tokenizer.eos_token_id,\n",
    "  )[:, CONFIG['ngram_len'] - 1 :]\n",
    "\n",
    "  # context repetition mask is computed\n",
    "  context_repetition_mask = logits_processor.compute_context_repetition_mask(\n",
    "      input_ids=outputs,\n",
    "  )\n",
    "  # context repitition mask shape [batch_size, output_len - (ngram_len - 1)]\n",
    "\n",
    "  combined_mask = context_repetition_mask * eos_token_mask\n",
    "\n",
    "  g_values = logits_processor.compute_g_values(\n",
    "      input_ids=outputs,\n",
    "  )\n",
    "  # g values shape [batch_size, output_len - (ngram_len - 1), depth]\n",
    "  del model, inputs\n",
    "\n",
    "  return g_values, combined_mask\n",
    "\n",
    "\n",
    "example_inputs = [\n",
    "    'I enjoy walking with my cute dog',\n",
    "    'I am from New York',\n",
    "    'The test was not so very hard after all',\n",
    "    \"I don't think they can score twice in so short a time\",\n",
    "]\n",
    "\n",
    "wm_g_values, wm_mask = generate_responses(\n",
    "    example_inputs, enable_watermarking=True\n",
    ")\n",
    "uwm_g_values, uwm_mask = generate_responses(\n",
    "    example_inputs, enable_watermarking=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPzN6_SInzyG"
   },
   "source": [
    "# Mean detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "KMU5Ut7Gnng9"
   },
   "outputs": [],
   "source": [
    "# Watermarked responses tend to have higher Mean scores than unwatermarked\n",
    "# responses. To classify responses you can set a score threshold, but this will\n",
    "# depend on the distribution of scores for your use-case and your desired false\n",
    "# positive / false negative rates.\n",
    "\n",
    "wm_mean_scores = detector_mean.mean_score(\n",
    "    wm_g_values.cpu().numpy(), wm_mask.cpu().numpy()\n",
    ")\n",
    "uwm_mean_scores = detector_mean.mean_score(\n",
    "    uwm_g_values.cpu().numpy(), uwm_mask.cpu().numpy()\n",
    ")\n",
    "\n",
    "print('Mean scores for watermarked responses: ', wm_mean_scores)\n",
    "print('Mean scores for unwatermarked responses: ', uwm_mean_scores)\n",
    "\n",
    "# Wefind that the Weighted Mean scoring function gives better\n",
    "# classification performance than the Mean scoring function (in particular,\n",
    "# higher scores for watermarked responses). See the paper for full details.\n",
    "\n",
    "wm_weighted_mean_scores = detector_mean.weighted_mean_score(\n",
    "    wm_g_values.cpu().numpy(), wm_mask.cpu().numpy()\n",
    ")\n",
    "uwm_weighted_mean_scores = detector_mean.weighted_mean_score(\n",
    "    uwm_g_values.cpu().numpy(), uwm_mask.cpu().numpy()\n",
    ")\n",
    "\n",
    "print(\n",
    "    'Weighted Mean scores for watermarked responses: ', wm_weighted_mean_scores\n",
    ")\n",
    "print(\n",
    "    'Weighted Mean scores for unwatermarked responses: ',\n",
    "    uwm_weighted_mean_scores,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
