# NLPA-project
This repository contains all notebooks related to the Natural Language Processing Advanced (NLPA) course project of the group with the following members:
- Abel ANDRY
- Guillaume LALIRE
- Emile MERLE
- Julien SCHAFFAUSER
- Lilian SCHALL

## Watermarking text generated by LLMs with SynthID

Unidirectional Transformers generate text by sequentially predicting each word,
based on probabilities calculated from the preceding context.
These probabilities are derived from the model’s logits,
which associate each word in the vocabulary with a probability of occurrence.

In a standard system, these probabilities are influenced solely by the context.
However, watermarking algorithms like SynthID-Text introduce a controlled bias into
this selection process by leveraging a specific key.

This bias can slightly increase or decrease the probabilities of certain words,
depending on their priority as defined by the key.

Later, to detect the watermark, the generated text is analyzed to verify if the word choices
or structures align with the “fingerprints” left by the key.
The SynthID-Text algorithm introduces an improvement in sampling with a new approach called Tournament Sampling.
The algorithm selects a token that scores highly against random marking functions.
This process can be visualized as a single-elimination tournament,
where tokens derived from the logits compete until only one winner remains.

At each generation step, SynthID-Text creates a random seed.
This generation is based on hashing the last few tokens and the watermarking key.
(In the paper, only the last four tokens are considered, so the seed mainly depends on the key).
A set of tokens is selected from the main logits of the LLM. These tokens then participate in the tournament.
At each round, two tokens compete based on the seed (with a new generation each time),
and are assigned a value (called g-value). The token with the higher value wins.

In the study, the DeepMind team used GPT-2, Gemma 7B, and Gemma 2B models.
On our side, we aim to evaluate the model's effectiveness on smaller models.
We then modify the source code to add the OLMo 1B and LLama 3.2 1B models.